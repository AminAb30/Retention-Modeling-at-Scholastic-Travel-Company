---
title: "IDS572-HW4"
author: "Amin Abbasi"
date: "2024-11-23"
output:
  pdf_document: default
  html_document: default
---

```{#r}

install.packages("smotefamily")
install.packages("DMwR2")
install.packages("caret")
install.packages("rpart")   # Install and load rpart for decision trees
install.packages("randomForest") # Install and load randomForest
install.packages("e1071") # Install and load e1071 for Naive Bayes
install.packages("adabag")  # Install and load adabag for AdaBoost
```


```{r}
library(readxl)  # For reading Excel files
library(smotefamily)
library(DMwR2)
library(caret)
library(ggplot2)
library(reshape2)
library(rpart)
library(pROC) # for ROC curve
library(randomForest)
library(e1071)
library(adabag)
```

##Importing the Dataset

```{r}

# Load your dataset (replace 'your_dataset.csv' with your actual dataset file)
data <- read_excel("Retention Modeling at Scholastic Travel Company (A)_ Student Spreadsheet.xlsx", sheet = "Data")

# Convert the target variable to a factor
data$Retained.in.2012. <- as.factor(data$Retained.in.2012.)


```
##Exploring the dataset

```{r}


# View the structure of the dataset
str(data)

# Get a summary of the dataset
summary(data)

```

##Preprocessing Data

#Remove Columns with Date Values
#Replace NA and "NA" (as text) with "" in all columns except "Special.Pay"
```{r}
# Remove specific columns by name
data <- data[, !names(data) %in% c("Return.Date", "Deposit.Date", "Departure.Date", "Early.RPL", "Latest.RPL", "FirstMeeting", "LastMeeting", "Initial.System.Date","ID" )]

# Check the structure of the updated dataset
#str(data)  # Confirm that the columns have been removed


# Replace "NA" and "" with NA in all columns
data <- data.frame(lapply(data, function(column) {
  # Replace empty strings or "NA" (string) with actual NA
  column[column == "" | column == "NA"] <- NA
  return(column)
}), stringsAsFactors = FALSE)


```


#Handling Missing Values
Check Missing Values in Each Column
```{r}
# Count missing values in each column
colSums(is.na(data))

```


#handle missing values in the dataset by replacing:

Missing values in numerical columns with the mean.
Missing values in categorical columns with the mode.

```{r}
# Load the dataset
data <- read.csv("Step1_removeNAs.csv", stringsAsFactors = FALSE)

# Function to calculate the mode
calculate_mode <- function(column) {
  unique_values <- unique(column)
  unique_values <- unique_values[!is.na(unique_values)]  # Exclude NA from calculation
  mode_value <- unique_values[which.max(tabulate(match(column, unique_values)))]
  return(mode_value)
}

# Separate numeric and categorical columns
numeric_columns <- sapply(data, is.numeric)
categorical_columns <- sapply(data, is.character)

# Replace missing values in numeric columns with the mean
data[numeric_columns] <- lapply(data[numeric_columns], function(column) {
  ifelse(is.na(column), median(column, na.rm = TRUE), column)
})

# Replace missing values in categorical columns with the mode
data[categorical_columns] <- lapply(data[categorical_columns], function(column) {
  column[is.na(column)] <- calculate_mode(column)
  return(column)
})

# Total number of missing values in the dataset
total_missing <- sum(is.na(data))

# Print the result
print(paste("Total number of missing values in the dataset:", total_missing))
```
Now there are no Missing Value in the dataset

#Handling Outliers


#Cap Outliers 
cap the values at the IQR thresholds
```{r}
handle_outliers_iqr_cap <- function(data) {
  numeric_columns <- sapply(data, is.numeric)  # Identify numeric columns
  
  data[numeric_columns] <- lapply(data[numeric_columns], function(column) {
    if (is.numeric(column)) {
      Q1 <- quantile(column, 0.25, na.rm = TRUE)
      Q3 <- quantile(column, 0.75, na.rm = TRUE)
      IQR <- Q3 - Q1
      lower_bound <- Q1 - 1.5 * IQR
      upper_bound <- Q3 + 1.5 * IQR
      
      # Cap the values at lower and upper bounds
      column[column < lower_bound] <- lower_bound
      column[column > upper_bound] <- upper_bound
    }
    return(column)
  })
  
  return(data)
}

# Apply the capping method
data_capped <- handle_outliers_iqr_cap(data)

# Visualize boxplots again
library(ggplot2)
numeric_columns <- names(data_capped)[sapply(data_capped, is.numeric)]
for (column_name in numeric_columns) {
  print(
    ggplot(data_capped, aes_string(y = column_name)) +
      geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
      labs(title = paste("Boxplot of", column_name, "After Capping"), y = column_name) +
      theme_minimal()
  )
}


```
Successfully Handled the Outliers 

#handeling Rare Values in Cat columns
Identify Rare Categories
```{r}
#  threshold 0.5%  for rare categories
threshold <- 0.005

# Identify rare categories for each categorical column
categorical_columns <- sapply(data, is.character)  # Identify categorical columns
rare_categories <- lapply(data[, categorical_columns], function(column) {
  freq_table <- prop.table(table(column))
  names(freq_table[freq_table < threshold])  # Return rare categories
})

# Print rare categories for each column
rare_categories

```

Handle Rare Categories by group Rare Categories into "Other": Combine rare categories into a single group labeled "Other".
```{r}
# Combine rare categories into "Other"
data[, categorical_columns] <- lapply(names(data)[categorical_columns], function(col_name) {
  column <- data[[col_name]]
  freq_table <- prop.table(table(column))
  rare_values <- names(freq_table[freq_table < threshold])
  column[column %in% rare_values] <- "Other"
  return(column)
})

```


```{r}
# Check the frequency distribution after handling rare values
for (col_name in names(data)[categorical_columns]) {
  print(table(data[[col_name]]))
}

```

#Check Balance
```{r}
# Check the class distribution of the target variable
table(data$Retained.in.2012.)

# Calculate the proportion of each class
prop.table(table(data$Retained.in.2012.))

# Visualize the class distribution
library(ggplot2)

ggplot(data, aes(x = as.factor(Retained.in.2012.))) +
  geom_bar(fill = "skyblue") +
  labs(title = "Class Distribution of Retained.in.2012.", x = "Class", y = "Count") +
  theme_minimal()

```
The dataset has some imbalance, but it is not severe (about a 60-40 split) and logistic regression, decision trees, and random forests can handle mild imbalances reasonably well, However Algorithms like neural networks may perform better with balanced data.Additionally because the cost of misclassifying the minority class (0) is high and we must use  Evaluate performance  like precision, recall, F1-score, and ROC-AUC, I consider balancing.

```{r}


# Apply undersampling
downsampled_data <- downSample(x = data[, -which(names(data) == "Retained.in.2012.")],
                               y = as.factor(data$Retained.in.2012.))

# Check the new class distribution
table(downsampled_data$Class)


```
Now the data is balanced


#Normalization

I dicided to Normalize the data because there are some features with large values (e.g., tuition) can dominate the model's calculations, overshadowing features with smaller values (e.g., rates) to ensures that all features contribute equally to the model.

I used Min-Max Normalization and In the his scales all features to a range between [0, 1]:
```{r}
# Min-Max Normalization
normalize_minmax <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Apply normalization to numeric columns
numeric_columns <- sapply(data, is.numeric)
data[numeric_columns] <- lapply(data[numeric_columns], normalize_minmax)

```


```{r}
str(data)
```

##Correlations

Calculate Correlations:
```{r}
# Select only numeric columns (excluding the target variable)
numeric_columns <- names(data)[sapply(data, is.numeric)]
numeric_features <- numeric_columns[numeric_columns != "Retained.in.2012."]

# Compute the correlation matrix
correlation_matrix <- cor(data[, numeric_features], use = "pairwise.complete.obs")

# Find highly correlated pairs (correlation > 0.8 or < -0.8)
high_corr_pairs <- which(abs(correlation_matrix) > 0.8, arr.ind = TRUE)

# Exclude diagonal elements (self-correlations)
high_corr_pairs <- high_corr_pairs[high_corr_pairs[, 1] != high_corr_pairs[, 2], ]

# Display the highly correlated pairs
if (nrow(high_corr_pairs) > 0) {
  cat("Highly correlated pairs:\n")
  for (i in 1:nrow(high_corr_pairs)) {
    cat(rownames(correlation_matrix)[high_corr_pairs[i, 1]], 
        "and", 
        colnames(correlation_matrix)[high_corr_pairs[i, 2]], 
        "with correlation:", 
        correlation_matrix[high_corr_pairs[i, 1], high_corr_pairs[i, 2]], 
        "\n")
  }
} else {
  cat("No highly correlated pairs found.\n")
}

```

Other Variables Correlation with the Target Variable "Retained.in.2012."
```{r}
# Compute correlations of numeric features with the target variable
target_correlations <- sapply(numeric_features, function(col) {
  cor(data[[col]], data$Retained.in.2012., use = "pairwise.complete.obs")
})

# Sort features by their correlation with the target variable
sorted_correlations <- sort(abs(target_correlations), decreasing = TRUE)

# Display correlations with the target
cat("Correlation with Retained.in.2012.:\n")
print(sorted_correlations)

```

Visualize Correlations
```{r}
# Melt the correlation matrix for visualization
correlation_melted <- melt(correlation_matrix)

# Plot the heatmap
ggplot(correlation_melted, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1), space = "Lab") +
  labs(title = "Feature Correlation Heatmap", x = "Features", y = "Features") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
#Remove Highly Correlated Features
```{r}
# Keep only one feature from each highly correlated pair based on target correlation
features_to_remove <- c()
for (i in 1:nrow(high_corr_pairs)) {
  feature1 <- rownames(correlation_matrix)[high_corr_pairs[i, 1]]
  feature2 <- colnames(correlation_matrix)[high_corr_pairs[i, 2]]
  
  # Compare correlations with the target and decide which to keep
  if (abs(target_correlations[feature1]) >= abs(target_correlations[feature2])) {
    features_to_remove <- c(features_to_remove, feature2)
  } else {
    features_to_remove <- c(features_to_remove, feature1)
  }
}

# Remove Duplicates
features_to_remove <- unique(features_to_remove)

# Drop Features from the Dataset
data <- data[, !names(data) %in% features_to_remove]

cat("\nRemoved features due to high correlation:\n")
print(features_to_remove)

```

#Drop Low Correlation Features
```{r}
# Drop features with low correlation to the target variable
low_correlation_features <- names(sorted_correlations[sorted_correlations < 0.1])
low_correlation_features

data <- data[, !names(data) %in% low_correlation_features]

```

#Corelations after dropping
```{r}

# Select only numeric columns (excluding the target variable)
numeric_columns <- names(data)[sapply(data, is.numeric)]
numeric_features <- numeric_columns[numeric_columns != "Retained.in.2012."]

# Compute the correlation matrix
correlation_matrix <- cor(data[, numeric_features], use = "pairwise.complete.obs")

# Find highly correlated pairs (correlation > 0.8 or < -0.8)
high_corr_pairs <- which(abs(correlation_matrix) > 0.8, arr.ind = TRUE)

# Exclude diagonal elements (self-correlations)
high_corr_pairs <- high_corr_pairs[high_corr_pairs[, 1] != high_corr_pairs[, 2], ]

# Display the highly correlated pairs
if (nrow(high_corr_pairs) > 0) {
  cat("Highly correlated pairs:\n")
  for (i in 1:nrow(high_corr_pairs)) {
    cat(rownames(correlation_matrix)[high_corr_pairs[i, 1]], 
        "and", 
        colnames(correlation_matrix)[high_corr_pairs[i, 2]], 
        "with correlation:", 
        correlation_matrix[high_corr_pairs[i, 1], high_corr_pairs[i, 2]], 
        "\n")
  }
} else {
  cat("No highly correlated pairs found.\n")
}


# Compute correlations of numeric features with the target variable
target_correlations <- sapply(numeric_features, function(col) {
  cor(data[[col]], data$Retained.in.2012., use = "pairwise.complete.obs")
})

# Sort features by their correlation with the target variable
sorted_correlations <- sort(abs(target_correlations), decreasing = TRUE)

# Display correlations with the target
cat("Correlation with Retained.in.2012.:\n")
print(sorted_correlations)

# Melt the correlation matrix for visualization
correlation_melted <- melt(correlation_matrix)

# Plot the heatmap
ggplot(correlation_melted, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "green", high = "orange", mid = "white", midpoint = 0, limit = c(-1, 1), space = "Lab") +
  labs(title = "Feature Correlation Heatmap", x = "Features", y = "Features") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

#Correlation with the Target Variable after remove high and low corelation variables
```{r}
# Compute correlations of numeric features with the target variable
target_correlations <- sapply(numeric_features, function(col) {
  cor(data[[col]], data$Retained.in.2012., use = "pairwise.complete.obs")
})

# Sort features by their correlation with the target variable
sorted_correlations <- sort(abs(target_correlations), decreasing = TRUE)

# Display correlations with the target
cat("Correlation with Retained.in.2012.:\n")
print(sorted_correlations)
```


#Univariate Analysis

Descriptive Statistics
```{r}
# Descriptive statistics for numeric variables
summary(data)

# View unique values for categorical variables
sapply(data, function(col) if (is.factor(col) || is.character(col)) unique(col))

```
Visualizing Distributions
For Numeric Variables: Use histograms or density plots.
```{r}

# Identify numeric and character columns
numeric_columns <- names(data)[sapply(data, is.numeric)]
character_columns <- names(data)[sapply(data, is.character)]

# Step 1: Process Numeric Columns (e.g., Histograms, Descriptive Statistics)
cat("Processing Numeric Columns:\n")
for (col_name in numeric_columns) {
  cat("\nSummary of", col_name, ":\n")
  print(summary(data[[col_name]]))  # Print summary statistics
  
  # Plot histogram
  print(
    ggplot(data, aes_string(x = col_name)) +
      geom_histogram(fill = "blue", color = "black", bins = 30) +
      labs(title = paste("Histogram of", col_name), x = col_name, y = "Frequency") +
      theme_minimal()
  )
}

# Step 2: Process Character Columns (e.g., Frequency Counts, Bar Plots)
cat("\nProcessing Character Columns:\n")
for (col_name in character_columns) {
  cat("\nFrequency Table of", col_name, ":\n")
  print(table(data[[col_name]], useNA = "ifany"))  # Print frequency table
  
  # Plot bar chart
  print(
    ggplot(data, aes_string(x = col_name)) +
      geom_bar(fill = "green", color = "black") +
      labs(title = paste("Bar Plot of", col_name), x = col_name, y = "Count") +
      theme_minimal()
  )
}


```
#Multivariate Analysis
Correlation Analysis
Computed pairwise correlations between numeric variables.
```{r}
# Correlation matrix
correlation_matrix <- cor(data[sapply(data, is.numeric)], use = "pairwise.complete.obs")

# Heatmap for visualization
library(reshape2)
correlation_melted <- melt(correlation_matrix)
ggplot(correlation_melted, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1)) +
  labs(title = "Correlation Heatmap", x = "Features", y = "Features") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```
#Scatter Plots
relationships between "two numeric variables"DifferenceTraveltoFirstMeeting" and "Retained.in.2012.".

```{r}
# Scatter plot for two numeric variables
ggplot(data, aes(x = DifferenceTraveltoFirstMeeting, y = Retained.in.2012.)) +
  geom_point(color = "blue") +
  labs(title = "Scatter Plot", x = "DifferenceTraveltoFirstMeeting", y = "Retained.in.2012.") +
  theme_minimal()

```

Inferential Statistics

#T_test
T-Test for All Numeric Variables
```{r}
# Ensure the target variable is a factor (required for grouping in t-test)
data$Retained.in.2012. <- as.factor(data$Retained.in.2012.)

# Identify numeric columns
numeric_columns <- names(data)[sapply(data, is.numeric)]

# Perform T-test for each numeric column
t_test_results <- lapply(numeric_columns, function(col) {
  test <- t.test(data[[col]] ~ data$Retained.in.2012., data = data)
  list(
    Variable = col,
    P_Value = test$p.value,
    Mean_Group_0 = test$estimate[1],
    Mean_Group_1 = test$estimate[2]
  )
})

# Convert results to a data frame for easy interpretation
t_test_summary <- do.call(rbind, lapply(t_test_results, as.data.frame))
t_test_summary <- as.data.frame(t_test_summary)

# View results
print(t_test_summary)

# Save results to a CSV file
write.csv(t_test_summary, "T_Test_Results.csv", row.names = FALSE)


```
Based on the low p-values, variables like To.Grade, Is.Non.Annual., FPP, and SingleGradeTripFlag should be considered important predictors in further modeling or analysis.Variables with the largest mean differences (e.g., SingleGradeTripFlag, FPP) might have the strongest influence.Since all variables have significant p-values, I retain them all for modeling.

```{r}
# Perform ANOVA for each numeric column
anova_results <- lapply(numeric_columns, function(col) {
  formula <- as.formula(paste(col, "~ Retained.in.2012."))
  result <- aov(formula, data = data)
  summary_result <- summary(result)
  
  # Extract the p-value
  p_value <- summary_result[[1]]["Pr(>F)"][1]
  
  # Return variable, F-statistic, and p-value
  list(
    Variable = col,
    F_Value = summary_result[[1]]["F value"][1],
    P_Value = p_value
  )
})

# Convert results to a data frame
anova_summary <- do.call(rbind, lapply(anova_results, as.data.frame))
anova_summary <- as.data.frame(anova_summary)

# View results
print(anova_summary)

# Save results to a CSV file
write.csv(anova_summary, "ANOVA_Test_Results.csv", row.names = FALSE)

```
All the variables listed are statistically significant because their p-values (Pr(>F)) are extremely small (e.g., 1.038787e-21, 3.802842e-88, etc.). SingleGradeTripFlag (F-value: 628.68, p-value: 2.273018e-123) has the highest F-value, suggesting the strongest group separation.
```{r}


```

##Models to Apply

#Train-Test Split
```{r}


# Split data into training and testing sets (80-20 split)
set.seed(123)
train_index <- createDataPartition(data$Retained.in.2012., p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

```
# Model Training and Evaluation
#Decision Tree:
```{r}


# Train Decision Tree
dt_model <- rpart(Retained.in.2012. ~ ., data = train_data, method = "class")

# Predict on test data
dt_predictions <- predict(dt_model, test_data, type = "class")

# Evaluate using confusion matrix
confusionMatrix(dt_predictions, test_data$Retained.in.2012.)

# Get predicted probabilities for the positive class (e.g., class `1`)
dt_probabilities <- predict(dt_model, test_data, type = "prob")[, 2]

# Plot ROC Curve
dt_roc <- roc(test_data$Retained.in.2012., dt_probabilities)
plot(dt_roc, col = "blue", main = "ROC Curve for Decision Tree")
auc(dt_roc)  # Print AUC value

```
Decision Tree

Strengths:
Highest accuracy (0.805) and precision (0.8013), meaning it performs slightly better overall in correctly classifying both classes.
Highest specificity (0.8931): Best at correctly identifying class 1.
Balanced accuracy (0.7808) indicates good performance on both classes.

Weaknesses:
Lowest sensitivity (0.6684): Performs poorly in identifying class 0.
McNemar's test p-value (0.001865) suggests significant bias, likely favoring class 1.
Lowest AUC (0.8062): Indicates relatively weaker ability to discriminate between classes.





#random Forest
```{r}


# Train Random Forest
rf_model <- randomForest(Retained.in.2012. ~ ., data = train_data)

# Predict on test data
rf_predictions <- predict(rf_model, test_data)

# Evaluate using confusion matrix
confusionMatrix(rf_predictions, test_data$Retained.in.2012.)


# Get predicted probabilities for the positive class (e.g., class `1`)
rf_probabilities <- predict(rf_model, test_data, type = "prob")[, 2]

# Plot ROC Curve
rf_roc <- roc(test_data$Retained.in.2012., rf_probabilities)
plot(rf_roc, col = "green" , main = "ROC Curve for randow forest")  # Add to the same plot

auc(rf_roc)  # Print AUC value


```
Random Forest

Strengths:
Best AUC (0.87), indicating the strongest ability to discriminate between classes.
High specificity (0.8621): Effective at correctly identifying class 1.
Balanced accuracy (0.7733) is higher than Logistic Regression but slightly lower than Naive Bayes.

Weaknesses:
Moderate sensitivity (0.6845): Similar to Logistic Regression but misses ~31.5% of actual class 0 cases.
McNemar's test p-value (0.07044) is marginally significant, indicating slight class imbalance in predictions.

#Naive Bayes:

```{r}


# Train Naive Bayes
nb_model <- naiveBayes(Retained.in.2012. ~ ., data = train_data)

# Predict on test data
nb_predictions <- predict(nb_model, test_data)

# Evaluate using confusion matrix
confusionMatrix(nb_predictions, test_data$Retained.in.2012.)

# Get predicted probabilities for the positive class (e.g., class `1`)
nb_probabilities <- predict(nb_model, test_data, type = "raw")[, 2]

# Plot ROC Curve
nb_roc <- roc(test_data$Retained.in.2012., nb_probabilities)
plot(nb_roc, col = "red", main = "ROC Curve for Naive Bayes")  # Add to the same plot
auc(nb_roc)  # Print AUC value


```
Naive Bayes Analyse

Strengths:
Highest sensitivity (0.7059): Best at identifying class 0 cases.
AUC (0.8507) slightly edges out Logistic Regression.
Balanced accuracy (0.7771) is higher than Logistic Regression.
McNemar's test p-value (0.3149): No significant class bias.

Weaknesses:
Slightly lower precision (0.7500): More false positives compared to other models.
Specificity (0.8483) is lower than other models, indicating slightly less accuracy in identifying class 1.

#Logistic Regression

```{r}
# Train Logistic Regression
lr_model <- glm(Retained.in.2012. ~ ., data = train_data, family = "binomial")

# Predict probabilities on test data
lr_probabilities <- predict(lr_model, test_data, type = "response")

# Convert probabilities to class predictions
lr_predictions <- ifelse(lr_probabilities > 0.5, 1, 0)

# Evaluate using confusion matrix
confusionMatrix(as.factor(lr_predictions), test_data$Retained.in.2012.)

# Use predicted probabilities from the logistic regression model
lr_probabilities <- predict(lr_model, test_data, type = "response")

# Plot ROC Curve
lr_roc <- roc(test_data$Retained.in.2012., lr_probabilities)
plot(lr_roc, col = "purple", main = "ROC Curve for Logistic Regression")  # Add to the same plot
auc(lr_roc)  # Print AUC value


```
Logistic Regression analyse

__Strengths:
AUC of 0.8504 shows strong discrimination capability.
High specificity (0.8655): Effective in correctly identifying class 1.
Balanced accuracy (0.7723) indicates reasonable performance across both classes.
__Weaknesses:
Moderate sensitivity (0.6791): Misses ~32% of actual class 0 cases.
McNemar's test p-value (0.04442) suggests a significant difference in error rates between classes, indicating potential bias.



```{r}
# Plot Decision Tree ROC
plot(dt_roc, col = "blue", main = "ROC Curves for All Models", lwd = 2)

# Add other models' ROC curves
plot(rf_roc, col = "green", add = TRUE, lwd = 2)
plot(nb_roc, col = "red", add = TRUE, lwd = 2)
plot(lr_roc, col = "purple", add = TRUE, lwd = 2)


# Add a legend
legend("bottomright", legend = c("Decision Tree", "Random Forest", "Naive Bayes", "Logistic Regression"),
       col = c("blue", "green", "red", "purple"), lwd = 2)


```
ROC Curve Analyst:
The ROC curve compares the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) for each model. 

Decision Tree (Blue):

This curve is noticeably lower than the others, indicating it is the least effective model in separating the two classes.
The AUC for this model is the smallest, reflecting relatively weaker performance compared to the others.
Random Forest (Green):

The Random Forest curve is the highest among all models, suggesting it achieves the best trade-off between sensitivity and specificity.
This model likely has the highest AUC, as it consistently outperforms the others across all thresholds.
Naive Bayes (Red):

The Naive Bayes curve closely follows Random Forest and Logistic Regression.
Its performance is slightly lower than Random Forest but still competitive, with a strong AUC score.
Logistic Regression (Purple):

Logistic Regression is similar to Naive Bayes and slightly below Random Forest.
The AUC is competitive, though slightly lower than Random Forest, indicating strong but not the best overall performance.

-----------------------------------------------------------------------------------------

>> Best Model: Random Forest
The Random Forest model has the highest AUC (0.87), strong accuracy (0.7925), and high specificity (0.8621).
It balances sensitivity and specificity better than Logistic Regression and Decision Tree, making it a reliable choice for this classification task.

>> Secondary Choice: Naive Bayes
Naive Bayes achieves the highest sensitivity (0.7059) and a competitive AUC (0.8507), making it a good option if identifying class 0 (minimizing false negatives) is critical.
```{r}


```

```{r}


```

```{r}


```

```{r}


```

